{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cover.jpg'alt=\"Matrix\" height=\"full\" width=\"full\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors \n",
    " \n",
    "What is Vector?\n",
    "A n-dimensional vector y is an ordered collection of k real numbers y1 , y2 , . . . , yn, and is written as \n",
    "y = (y1,y2,...,yn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectors\n",
    "\n",
    "y = [1,2,3,4,5,6]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#using numpy\n",
    "import numpy as np\n",
    "\n",
    "x = [1,2,3,4,5,6]\n",
    "y = np.array([1,2,3,4,5,6])\n",
    "print(x)\n",
    "print(type(x))\n",
    "print(type(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices\n",
    "\n",
    "A Matrix is defined to be a rectangular array of numbers. Whose dimension is m by n. A is called square if m = n. The numbers ai j are referred to as the elementsof A.\n",
    "\n",
    "<img src='matrix.png' alt=\"Matrix\" height=\"300\" width=\"300\" align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 5 3]\n",
      " [4 7 5]\n",
      " [7 5 6]]\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "# matrices\n",
    "\n",
    "m = np.array([[1,5,3],\n",
    "             [4,7,5],\n",
    "             [7,5,6]])\n",
    "\n",
    "# preview matrix\n",
    "print(m)\n",
    "\n",
    "# view dimensions\n",
    "print(m.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Transpose \n",
    "\n",
    "<img src='trans.jpg' align='left'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Transpose:\n",
      " [[1 4 7]\n",
      " [5 7 5]\n",
      " [3 5 6]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# matrix transpose\n",
    "\n",
    "print('Matrix Transpose:\\n',m.transpose(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Determinant\n",
    "\n",
    "<img src='determinant.png' align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Determinant: -15.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# matrix determinant\n",
    "\n",
    "print ('Matrix Determinant:',np.linalg.det(m),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Inverse\n",
    "\n",
    "<img src='inverse.png'align='left' >\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Inverse:\n",
      " [[-1.13333333  1.         -0.26666667]\n",
      " [-0.73333333  1.         -0.46666667]\n",
      " [ 1.93333333 -2.          0.86666667]]\n"
     ]
    }
   ],
   "source": [
    "# matrix inverse\n",
    "m_inv = np.linalg.inv(m)\n",
    "print('Matrix Inverse:\\n',m_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Matrix\n",
    "\n",
    "<img src='iden.png' align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product of matrix and its inverse:\n",
      " [[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# identity matrix (Original Matrix * Matrix Inverse)\n",
    "\n",
    "iden_m = np.dot(m, m_inv)\n",
    "iden_m = np.round(np.abs(iden_m), 0)\n",
    "\n",
    "print('Product of matrix and its inverse:\\n',iden_m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Decompostion\n",
    "\n",
    "Eigen means own or self. \n",
    "Eigendecomposition is the method to decompose a square matrix into its eigenvalues and eigenvectors. \n",
    "For a matrix A, if\n",
    "        Av=λv\n",
    "then \n",
    "v is an eigenvector of matrix A and \n",
    "λ is the corresponding eigenvalue. \n",
    "That is, if matrix A is multiplied by a vector and the result is a scaled version of the same vector, then it is an eigenvector of A and the scaling factor is its eigenvalue.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen Value: [ -1.53771503  10.70683037   5.83088467] \n",
      "\n",
      "Eigen Vector:\n",
      " [[ 0.88691954 -0.43837763 -0.04359003]\n",
      " [-0.29794271 -0.85425723 -0.63258938]\n",
      " [-0.35299273 -0.27940945  0.77325971]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "\n",
    "m = np.array([[1,4,3],\n",
    "              [4,8,2],\n",
    "              [3,0,6]])\n",
    "\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(m)\n",
    "\n",
    "print('Eigen Value:', eigen_vals, '\\n')\n",
    "print('Eigen Vector:\\n', eigen_vecs, '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD\n",
    "\n",
    "#####  SVD a.k.a Singular Value Decomposition is a matrix factorization method. Matrix Factorization is a representation of a matrix into a product of matrices. There are many different matrix factorization and each used for different class of problems.\n",
    "\n",
    "\n",
    "<img src='SVD.png' height=\"full\" width=\"full\" align='left'>  <br>\n",
    "A is an m × n matrix <br>\n",
    "U is an m × n orthogonal matrix <br>\n",
    "S is an n × n diagonal matrix <br>\n",
    "V is an n × n orthogonal matrix <br>\n",
    "\n",
    "##### The diagonal values of Σ are known as Singular values of M.\n",
    "Conjugate Transpose: The conjugate transpose of a matrix interchanges the row and column index for each element.<br>\n",
    "Identity matrix: It is a square matrix in which all the elements of the principal diagonal are ones and all other elements are zeros.<br>\n",
    "Diagonal Matrix: It is a matrix in which the entries outside the main diagonal are all zero.<br>\n",
    "Unitary matrix: Matrices whose conjugate transpose is also its inverse, that is UU*=I.<br>\n",
    "Singular Values: Basically it denotes the square root of the eigenvalues of XX* where X is a matrix <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting SVD outputs:-\n",
      "\n",
      "U:\n",
      " [[-0.20851336 -0.9652919   0.15726959]\n",
      " [-0.65219648  0.25706784  0.71313104]\n",
      " [-0.72880857  0.04612668 -0.68316206]] \n",
      "\n",
      "S:\n",
      " [ 17.40756876   0.93974996   0.3056466 ] \n",
      "\n",
      "VT:\n",
      " [[-0.32931271 -0.63313747 -0.70049275]\n",
      " [ 0.26335319 -0.77402232  0.57579037]\n",
      " [ 0.90675148 -0.00513808 -0.42163416]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVD\n",
    "m = np.array([[1, 3, 2],\n",
    "              [4, 7, 8],\n",
    "              [4, 8, 9]])\n",
    "\n",
    "U, S, VT = np.linalg.svd(m)\n",
    "\n",
    "print ('Getting SVD outputs:-\\n')\n",
    "print('U:\\n', U, '\\n')\n",
    "print('S:\\n', S, '\\n')\n",
    "print('VT:\\n', VT, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n",
    "Descriptive statistics uses the data to provide descriptions of the population, either through numerical calculations or graphs or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [ 3  7 18  2  7 29 12  5 24  9 21 11 29 11 10 17 29 25]\n"
     ]
    }
   ],
   "source": [
    "# descriptive statistics\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# get data\n",
    "nums = np.random.randint(1,30, size=(1,18))[0]\n",
    "print('Data: ', nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 14.9444444444\n",
      "Median: 11.5\n",
      "Mode: ModeResult(mode=array([29]), count=array([3]))\n",
      "Standard Deviation: 9.0090832079\n",
      "Variance: 81.1635802469\n",
      "Skew: 0.304707786888169\n",
      "Kurtosis: -1.2789935717947152\n"
     ]
    }
   ],
   "source": [
    "# get descriptive stats\n",
    "print ('Mean:', sp.mean(nums))\n",
    "print ('Median:', sp.median(nums))\n",
    "print ('Mode:', sp.stats.mode(nums))\n",
    "print ('Standard Deviation:', sp.std(nums))\n",
    "print ('Variance:', sp.var(nums))\n",
    "print ('Skew:', sp.stats.skew(nums))\n",
    "print ('Kurtosis:', sp.stats.kurtosis(nums))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Natural-language processing (NLP)\n",
    "\n",
    "A field of computer science, artificial intelligence concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language data.\n",
    "\n",
    "###### NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,” and “an amazing library to play with natural language.”\n",
    "\n",
    "\n",
    "<img src='dialogue.png' height='500' width='600' align='left'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4660f5e15294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_jars_within_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdep_parser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mStanfordDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mstanford_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stanford_jar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_regex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             ),\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         )\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    714\u001b[0m                     (name_pattern, url))\n\u001b[1;32m    715\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m def find_jar(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\n  NLTK was unable to find stanford-parser\\.jar! Set the CLASSPATH\n  environment variable.\n\n  For more information, on stanford-parser\\.jar, see:\n    <https://nlp.stanford.edu/software/lex-parser.shtml>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "import numpy as np\n",
    "from nltk.internals import find_jars_within_path\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "dep_parser=StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "stanford_dir = st._stanford_jar.rpartition('/')[0]\n",
    "\n",
    "sentence = \"\"\"On 10 March, I will be giving interview at IIT Ghandinagar \"\"\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens #tokens\n",
    "\n",
    "tagged = nltk.pos_tag(tokens) #\n",
    "#print(tagged)\n",
    "\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "#print(entities)\n",
    "scp = StanfordParser(path_to_jar='/Users/apple/Downloads/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                   path_to_models_jar='/Users/apple/Downloads/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "\n",
    "# get parse tree\n",
    "result = list(scp.raw_parse(sentence)) \n",
    "tree = result[0]\n",
    "\n",
    "# print the constituency parse tree\n",
    "print(tree)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the /Users/apple/Downloads/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-655ed150fe9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/apple/Downloads/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpos_tagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Add other jars from Stanford directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordPOSTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     68\u001b[0m         self._stanford_model = find_file(model_filename,\n\u001b[1;32m     69\u001b[0m                                          \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STANFORD_MODELS'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                          verbose=verbose)\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose)\u001b[0m\n\u001b[1;32m    573\u001b[0m         file_names=None, url=None, verbose=False):\n\u001b[1;32m    574\u001b[0m     return next(find_file_iter(filename, env_vars, searchpath,\n\u001b[0;32m--> 575\u001b[0;31m                                file_names, url, verbose))\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the /Users/apple/Downloads/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger file!\nUse software specific configuration paramaters or set the STANFORD_MODELS environment variable.\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.internals import find_jars_within_path\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Alternatively to setting the CLASSPATH add the jar and model via their path:\n",
    "jar = '/Users/apple/Downloads/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar'\n",
    "model = '/Users/apple/Downloads/stanford-postagger-full-2015-04-20/models/english-left3words-distsim.tagger'\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar)\n",
    "\n",
    "# Add other jars from Stanford directory\n",
    "stanford_dir = pos_tagger._stanford_jar.rpartition('/')[0]\n",
    "stanford_jars = find_jars_within_path(stanford_dir)\n",
    "pos_tagger._stanford_jar = ':'.join(stanford_jars)\n",
    "\n",
    "text = pos_tagger.tag(word_tokenize(\"What's the airspeed of an unladen swallow ?\"))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
