{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "The application of computational techniques to the analysis and synthesis of natural language and speech.\n",
    "\n",
    "It's an field of Computer science that deals with Computational Linguistics, Human Computer Interaction and Artifical Intelligence.\n",
    "\n",
    "\n",
    "import nltk #natural language processing lib for Python\n",
    "\n",
    "### Tokenizing. \n",
    "\n",
    "A form of chopping sequence character. \n",
    "1. Word Tokenizers. They do in words, Keywords , Name.\n",
    "\n",
    "2. Sentence Tokenizers. Eg. Phrases, Speechs\n",
    "\n",
    "### Lexicon and Corporas\n",
    "\n",
    "Corpora: Body of structured text. ex. cogs science papers, speeches\n",
    "\n",
    "Lexicon: Words/text  and their Meaning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Roy, how are you doing today?', 'Keep clam the interview is going very well.'] \n",
      "\n",
      "Hello\n",
      "Mr.\n",
      "Roy\n",
      ",\n",
      "how\n",
      "are\n",
      "you\n",
      "doing\n",
      "today\n",
      "?\n",
      "Keep\n",
      "clam\n",
      "the\n",
      "interview\n",
      "is\n",
      "going\n",
      "very\n",
      "well\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#learn about tokenize\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sample_text = \"Hello Mr. Roy, how are you doing today? Keep clam the interview is going very well.\"\n",
    "\n",
    "print(sent_tokenize(sample_text),'\\n')\n",
    "for i in word_tokenize(sample_text):\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stop_words: {'o', 'both', 'ma', 'their', 'they', 'who', 'will', 'then', 'her', 'ain', 'was', \"you've\", 'them', 'over', \"that'll\", 'him', 'itself', 'has', 'shouldn', 'these', 'those', 'now', 'between', 'll', 'against', 'being', 'were', 'too', 'where', 'own', 'so', 'as', 'other', 'yours', 'are', 'from', 'had', 'in', 'it', 'same', 'this', 'when', \"doesn't\", 'mustn', 'does', \"needn't\", 'all', 'mightn', 'isn', 'why', 'at', 'there', 'been', 'an', 'hasn', 'won', 'that', 'having', \"you'll\", 'theirs', 'off', 'no', 'did', 'into', 've', 'you', 'wasn', 'few', \"it's\", 'ourselves', 'shan', 'about', 'up', 'haven', 'ours', \"won't\", 'on', \"wasn't\", 'we', \"didn't\", 'above', 'because', 'any', 'don', 'am', 's', \"couldn't\", 'by', \"hadn't\", 'didn', \"shan't\", 'each', \"aren't\", 'and', 'his', 'out', 'yourself', 'down', 'under', 'below', 'y', 'just', 'doesn', 'needn', 'a', 'have', 'himself', 'm', 'should', 'while', 'hers', 'myself', 'after', 'the', 'herself', 'or', 'for', 'how', \"haven't\", \"mustn't\", 'do', 'to', 'weren', 'your', 'during', \"hasn't\", 'until', 'again', 'yourselves', 'before', \"wouldn't\", 't', \"should've\", 're', 'with', 'of', 'whom', 'be', \"shouldn't\", 'my', 'couldn', 'most', 'themselves', 'if', 'aren', \"mightn't\", 'more', 'i', 'its', 'd', \"you're\", 'doing', 'our', 'only', 'which', 'some', \"weren't\", 'nor', 'wouldn', 'he', 'through', 'hadn', 'such', 'can', 'here', 'once', 'what', 'further', 'is', \"she's\", 'very', 'not', 'than', \"you'd\", \"don't\", 'but', \"isn't\", 'she', 'me'} \n",
      "\n",
      "After filtering:  ['This', 'example', 'custom/filtered', 'words', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "#Learning Stop Words : Used for filiters, you need those words in our sentenc\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent=\"This is an example for the custom/filtered words in the sentence\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print('List of stop_words:',stop_words,'\\n')\n",
    "\n",
    "words = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "#for w in words:\n",
    "#   if w not in stop_words:\n",
    "#       filtered_sentence.append(w)\n",
    "\n",
    "filtered_sentence = [w for w in words if not w in stop_words] \n",
    "print('After filtering: ', filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learn\n",
      "learn\n",
      "learn\n",
      "learnli\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning Stemming \n",
    "# In linguistic Morphology and Information Retrievel, Stemming\n",
    "# Is a Process of reducing inflected words to their word stem\n",
    "# eg. I was taking a ride in a car. #eg. I was riding in the car.\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = ['learn','learning','learned','learnly','\\n\\n']\n",
    "\n",
    "for w in example_words:\n",
    "   print(ps.stem(w))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech List IN NLTK\n",
    "\n",
    "1. CC\t coordinating conjunctio\n",
    "2. CD\t cardinal digit\n",
    "3. DT\t determiner/\n",
    "4. EX\t existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "5. FW\t foreign word\n",
    "6. IN\t preposition/subordinating conjunction\n",
    "7. JJ\t adjective\t'big'\n",
    "8. JJR\t adjective, comparative\t'bigger'\n",
    "9. JJS\t adjective, superlative\t'biggest'\n",
    "10. LS\t list marker\t1)\n",
    "11. MD\t modal\tcould, will\n",
    "12. NN\t noun, singular 'desk'\n",
    "13. NNS\t noun plural\t'desks'\n",
    "14. NNP\t proper noun, singular\t'Harrison'\n",
    "15. NNPS\t proper noun, plural\t'Americans'\n",
    "16. PDT\t predeterminer\t'all the kids'\n",
    "17. POS\t possessive ending\tparent's\n",
    "18. PRP\t personal pronoun\tI, he, she\n",
    "19. PRP\t possessive pronoun\tmy, his, hers\n",
    "20. RB\t adverb\t very, silently,\n",
    "21. RBR\t adverb, comparative\tbetter\n",
    "22. RBS\t adverb, superlative\tbest\n",
    "23. RP\t particle\tgive up\n",
    "24. TO\t to\tgo 'to' the store.\n",
    "25. UH\t interjection\terrrrrrrrm\n",
    "26. VB\t verb, base form\ttake\n",
    "27. VBD\t verb, past tense\ttook\n",
    "28. VBG\t verb, gerund/present participle\ttaking\n",
    "29. VBN\tverb, past participle\ttaken\n",
    "30. VBP\t verb, sing. present, non-3d\ttake\n",
    "31. VBZ\t verb, 3rd person sing. present\ttakes\n",
    "32. WDT\t wh-determiner\twhich\n",
    "33. WP\t wh-pronoun\twho, what\n",
    "34. WP$\t possessive wh-pronoun\twhose\n",
    "35. WRB\t wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learn Named Entity Recognit\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
